from __future__ import annotations

import shutil
from pathlib import Path
from typing import Callable, TypedDict

import numpy as np
import yaml
from ase.data import chemical_symbols
from load_atoms import load_dataset, view
from load_atoms.atoms_dataset import DescribedDataset
from load_atoms.database.database_entry import DatabaseEntry, valid_licenses
from load_atoms.utils import lpad

# this file is at dev/scripts/pages.py
_PROJECT_ROOT = Path(__file__).parent.parent.parent
_DOC_SOURCE = _PROJECT_ROOT / "docs/source"
_DOWNLOAD_DIR = _PROJECT_ROOT / "testing-datasets"


def build_docs_page_for(name: str):
    # avoid downloading the dataset so that if local changes have been made,
    # they are reflected in the documentation preview
    shutil.copytree(
        _PROJECT_ROOT / "database" / name,
        _DOWNLOAD_DIR / name,
        dirs_exist_ok=True,
    )
    dataset = load_dataset(name, root=_DOWNLOAD_DIR)

    compute_info(dataset)  # type: ignore

    raw_rst = "\n\n".join(func(dataset) for func in _page_component_generators)  # type: ignore
    with open(_DOC_SOURCE / "datasets" / f"{name}.rst", "w") as f:
        f.write(raw_rst)


def build_datasets_index():
    # load all DatabaseEntry's
    entry_files = sorted((_PROJECT_ROOT / "database").glob("**/*.yaml"))
    entries = [DatabaseEntry.from_yaml_file(f) for f in entry_files]

    # 2. create table and place in database-summary.rst
    table = info_table(entries)

    header = """\
.. This file is autogenerated by dev/scripts/generate_page.py

Summary
=======

:code:`load_atoms` maintains a database of (named) datasets for ease of use. 
These can be downloaded from the internet (and cached locally) using 
:func:`~load_atoms.load_dataset`:

"""
    (_DOC_SOURCE / "database-summary.rst").write_text(header + table)

    # 3. build per-category pages
    toc = """\
.. toctree::
   :maxdepth: 1
   :hidden:
   :caption: Datasets

   database-summary
"""
    categories = sorted(set(e.category for e in entries))
    for category in categories:
        toc += f"   category/{category.lower().replace(' ', '-')}\n"

    (_DOC_SOURCE / "datasets-index.rst").write_text(toc)

    for category in categories:
        category_entries = [e for e in entries if e.category == category]
        title = (
            "#" * len(category) + "\n" + category + "\n" + "#" * len(category)
        )
        table = summary_cards(category_entries)
        toctree = """\
.. toctree::
    :maxdepth: 1
    :hidden:

"""
        for entry in category_entries:
            toctree += f"    ../datasets/{entry.name}\n"

        file_contents = f"""\
.. This file is autogenerated by dev/scripts/generate_page.py

{title}

{table}

{toctree}
"""
        (
            _DOC_SOURCE
            / "category"
            / f"{category.lower().replace(' ', '-')}.rst"
        ).write_text(file_contents)


ComponentGenerator = Callable[[DescribedDataset], str]
_page_component_generators: list[ComponentGenerator] = []


def register_component(func: ComponentGenerator):
    _page_component_generators.append(func)
    return func


@register_component
def auto_generated_warning(dataset: DescribedDataset) -> str:
    return ".. This file is autogenerated by dev/scripts/generate_page.py"


@register_component
def title(dataset: DescribedDataset) -> str:
    name = dataset.description.name
    return name + "\n" + "=" * len(name)


@register_component
def header(dataset: DescribedDataset) -> str:
    idx: int = dataset.description.representative_structure  # type: ignore
    if idx is None:
        # get first structure with more than 1 atom
        for i, s in enumerate(dataset):
            if len(s) > 1:
                idx = i
                break

    html_visualisation = view(dataset[idx], show_bonds=True)
    viz = '<div class="viz">' + html_visualisation.data + "</div>"

    file_name = f"_static/visualisations/{dataset.description.name}.html"
    (_DOC_SOURCE / "_static/visualisations").mkdir(exist_ok=True)
    with open(_DOC_SOURCE / file_name, "w") as f:
        f.write(viz)

    info = str(dataset.description.description).replace("\n", "\n        ")
    return f"""\
.. grid:: 1 1 2 2
    
    .. grid-item::

        .. raw:: html
            :file: ../{file_name}

    .. grid-item::
        :class: info-card

        {info}
"""


@register_component
def code_block(dataset: DescribedDataset) -> str:
    summary = str(dataset).replace("\n", "\n    ")
    return f"""\
.. code-block:: pycon

    >>> from load_atoms import load_dataset
    >>> load_dataset("{dataset.description.name}")
    {summary}
"""


@register_component
def license(dataset: DescribedDataset) -> str:
    if dataset.description.license is None:
        return ""

    license = dataset.description.license
    url = valid_licenses[license]
    return f"""\
License
-------

This dataset is licensed under the `{license} <{url}>`_ license.
"""


@register_component
def citation(dataset: DescribedDataset) -> str:
    if dataset.description.citation is None:
        return ""

    citation = dataset.description.citation.replace("\n", "\n    ")
    return f"""\
Citation
--------

If you use this dataset in your work, please cite the following:

.. code-block:: latex
    
    {citation}
"""


# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# save expensive information to avoid recomputation

_INFO_DIR = Path(__file__).parent / "info"
_INFO_DIR.mkdir(exist_ok=True)


class ComputedInfo(TypedDict):
    n_structures: int
    n_atoms: int
    elements: list[str]


def compute_info(dataset: DescribedDataset) -> ComputedInfo:
    Zs = np.unique(dataset.arrays["numbers"])
    elements = [chemical_symbols[Z] for Z in sorted(Zs)]

    d = dict(
        n_structures=len(dataset),
        n_atoms=dataset.n_atoms,
        elements=elements,
    )

    with open(
        _INFO_DIR / f"{dataset.description.name}-computed.yaml", "w"
    ) as f:
        yaml.dump(d, f)

    return d  # type: ignore


def get_info(dataset_name: str) -> ComputedInfo:
    if not (_INFO_DIR / f"{dataset_name}-computed.yaml").exists():
        dataset = load_dataset(dataset_name, root=_DOWNLOAD_DIR)
        return compute_info(dataset)  # type: ignore
    else:
        with open(_INFO_DIR / f"{dataset_name}-computed.yaml") as f:
            return yaml.safe_load(f)


def info_table(entries: list[DatabaseEntry]) -> str:
    header = """\
.. list-table::
    :header-rows: 1

    * - 
      - Elements
      - # Atoms
      - # Structures
      - License
      - Year
"""

    rows = []
    for entry in entries:
        entry_info = get_info(entry.name)

        if entry.license is None:
            license = "None"
        else:
            license = f"`{entry.license} <{valid_licenses[entry.license]}>`_"

        rows.append(
            f"""\
    * - :doc:`{entry.name} <../datasets/{entry.name}>`
      - {", ".join(entry_info["elements"])}
      - {entry_info["n_atoms"]:,}
      - {entry_info["n_structures"]:,}
      - {license}
      - {entry.year}"""
        )

    return header + "\n".join(rows)


def summary_cards(entries: list[DatabaseEntry]) -> str:
    parts = []
    for entry in entries:
        description = lpad(entry.description)
        parts.append(
            f"""
.. grid-item-card::
    :class-item: info-card

    .. centered:: :doc:`{entry.name} <../datasets/{entry.name}>`

{description}
"""
        )

    grid = """\
.. grid:: 2

""" + "\n".join(lpad(p) for p in parts)
    return grid
